
@book{jiang_deep_2019,
	address = {Singapore},
	edition = {1st ed. 2019.},
	title = {Deep {Learning} in {Object} {Detection} and {Recognition}, edited by {Xiaoyue} {Jiang}, {Abdenour} {Hadid}, {Yanwei} {Pang}, {Eric} {Granger}, {Xiaoyi} {Feng}.},
	isbn = {978-981-10-5152-4},
	abstract = {This book discusses recent advances in object detection and recognition using deep learning methods, which have achieved great success in the field of computer vision and image processing. It provides a systematic and methodical overview of the latest developments in deep learning theory and its applications to computer vision, illustrating them using key topics, including object detection, face analysis, 3D object recognition, and image retrieval. The book offers a rich blend of theory and practice. It is suitable for students, researchers and practitioners interested in deep learning, computer vision and beyond and can also be used as a reference book. The comprehensive comparison of various deep-learning applications helps readers with a basic understanding of machine learning and calculus grasp the theories and inspires applications in other computer vision tasks.},
	language = {eng},
	publisher = {Springer Singapore : Imprint: Springer},
	author = {Jiang, Xiaoyue and Hadid, Abdenour and Pang, Yanwei and Granger, Eric and Feng, Xiaoyi},
	year = {2019},
	keywords = {Data mining., Optical data processing., Pattern recognition.},
}

@misc{koehrsen_neural_2018,
	title = {Neural {Network} {Embeddings} {Explained}},
	note = {\url{https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526}},
	abstract = {How deep learning can represent War and Peace as a vector},
	language = {en},
	urldate = {2021-09-21},
	journal = {Medium},
	author = {Koehrsen, Will},
	month = oct,
	year = {2018},
}

@misc{saha_comprehensive_2018,
	title = {A {Comprehensive} {Guide} to {Convolutional} {Neural} {Networks} — the {ELI5} way},
	url = {https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53},
	abstract = {Artificial Intelligence has been witnessing a monumental growth in bridging the gap between the capabilities of humans and machines…},
	language = {en},
	urldate = {2021-09-21},
	journal = {Medium},
	author = {Saha, Sumit},
	month = dec,
	year = {2018},
}

@article{du_overview_2020,
	title = {Overview of two-stage object detection algorithms},
	volume = {1544},
	issn = {1742-6596},
	url = {https://doi.org/10.1088/1742-6596/1544/1/012033},
	doi = {10.1088/1742-6596/1544/1/012033},
	abstract = {Nowadays, object detection has gradually become a quite popular field. From the traditional methods to the methods used at this stage, object detection technology has made great progress, and is still continuously developing and innovating. This paper reviews two-stage object detection algorithms used at this stage, explaining in detail the working principles of Faster R-CNN, R-FCN, FPN, and Casecade R-CNN and analyzing the similarities and differences between these four two-stage object detection algorithms. Then we used HSRC2016 ship dataset to perform experiments with Faster R-CNN, R-FCN, FPN, and Casecade R-CNN and compared the effectiveness of them with experimental results.},
	language = {en},
	urldate = {2021-09-21},
	journal = {Journal of Physics: Conference Series},
	author = {Du, Lixuan and Zhang, Rongyu and Wang, Xiaotian},
	month = may,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {012033},
}

@misc{brownlee_gentle_2019,
	title = {A {Gentle} {Introduction} to the {Rectified} {Linear} {Unit} ({ReLU})},
	url = {https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/},
	abstract = {In a neural network, the activation function is responsible for transforming the summed weighted input from the node into the […]},
	language = {en-US},
	urldate = {2021-09-21},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = jan,
	year = {2019},
}

@article{girshick_rich_2014,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
	urldate = {2021-09-22},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = oct,
	year = {2014},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended version of our CVPR 2014 paper; latest update (v5) includes results using deeper networks (see Appendix G. Changelog)},
}

@article{uijlings_selective_2013,
	title = {Selective {Search} for {Object} {Recognition}},
	volume = {104},
	copyright = {Springer Science+Business Media New York 2013},
	issn = {09205691},
	url = {http://www.proquest.com/docview/1412093173/abstract/6D683BAB48F94F66PQ/1},
	doi = {http://dx.doi.org.kuleuven.e-bronnen.be/10.1007/s11263-013-0620-5},
	abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 \% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/{\textasciitilde}uijlings/SelectiveSearch.html).[PUBLICATION ABSTRACT]},
	language = {English},
	number = {2},
	urldate = {2021-09-22},
	journal = {International Journal of Computer Vision},
	author = {Uijlings, J. R. and R and van de Sande, K. E. and A and Gevers, T. and Smeulders, A. W. and M},
	month = sep,
	year = {2013},
	note = {Num Pages: 154-171
Place: New York, Netherlands
Publisher: Springer Nature B.V.},
	keywords = {Computer science, Computers--Computer Systems, Datasets, Image processing systems, Information management, Localization, Search engines, Software, Studies},
	pages = {154--171},
}

@article{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	note = {\url{http://arxiv.org/abs/1506.01497}},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	urldate = {2021-09-22},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended tech report},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore-ieee-org.kuleuven.e-bronnen.be/stamp/stamp.jsp?tp=&arnumber=7780460},
	urldate = {2021-09-22},
}

@inproceedings{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	doi = {10.1109/CVPR.2016.91},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Computer architecture, Microprocessors, Neural networks, Object detection, Pipelines, Real-time systems, Training},
	pages = {779--788},
}

@inproceedings{liu_ssd_2016,
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	volume = {9905},
	isbn = {978-3-319-46447-3},
	shorttitle = {{SSD}},
	doi = {10.1007/978-3-319-46448-0_2},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For {\textbackslash}(300 {\textbackslash}times 300{\textbackslash}) input, SSD achieves 74.3 \% mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for {\textbackslash}(512 {\textbackslash}times 512{\textbackslash}) input, SSD achieves 76.9 \% mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https:// github. com/ weiliu89/ caffe/ tree/ ssd.},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander},
	month = oct,
	year = {2016},
	pages = {21--37},
}

@book{leibe_computer_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Computer {Vision} – {ECCV} 2016: 14th {European} {Conference}, {Amsterdam}, {The} {Netherlands}, {October} 11–14, 2016, {Proceedings}, {Part} {I}},
	volume = {9905},
	isbn = {978-3-319-46447-3 978-3-319-46448-0},
	shorttitle = {Computer {Vision} – {ECCV} 2016},
	url = {http://link.springer.com/10.1007/978-3-319-46448-0},
	language = {en},
	urldate = {2021-09-24},
	publisher = {Springer International Publishing},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	doi = {10.1007/978-3-319-46448-0},
}

@misc{little_activation_2020,
	title = {Activation {Functions} ({Linear}/{Non}-linear) in {Deep} {Learning} — {ReLU}/{Sigmoid}/{SoftMax}/{Swish}/{Leaky} {ReLu}.},
	url = {https://xzz201920.medium.com/activation-functions-linear-non-linear-in-deep-learning-relu-sigmoid-softmax-swish-leaky-relu-a6333be712ea},
	abstract = {What is the activation function?},
	language = {en},
	urldate = {2021-09-24},
	journal = {Medium},
	author = {Little, Z²},
	month = may,
	year = {2020},
}

@article{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	urldate = {2021-09-24},
	author = {Girshick, Ross},
	month = sep,
	year = {2015},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in ICCV 2015},
}

@article{han_deep_2016,
	title = {Deep {Compression}: {Compressing} {Deep} {Neural} {Networks} with {Pruning}, {Trained} {Quantization} and {Huffman} {Coding}},
	shorttitle = {Deep {Compression}},
	url = {http://arxiv.org/abs/1510.00149},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
	urldate = {2021-09-26},
	author = {Han, Song and Mao, Huizi and Dally, William J.},
	month = feb,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Published as a conference paper at ICLR 2016 (oral)},
}

@misc{Facebook_PyTorch_2017,
	title = {{PyTorch}},
	note = {\url{https://www.PyTorch.org}},
	abstract = {An open source machine learning framework that accelerates the path from research prototyping to production deployment.},
	language = {en},
	urldate = {2021-10-02},
	author = {Adam~Paszke and Sam~Gross and Soumith~Chintala and Gregory~Chanan},
	year = {2017},
}

@misc{noauthor_tensorflow_nodate,
	title = {{TensorFlow}},
	url = {https://www.tensorflow.org/?hl=nl},
	abstract = {An end-to-end open source machine learning platform for everyone. Discover TensorFlow's flexible ecosystem of tools, libraries and community resources.},
	language = {en},
	urldate = {2021-10-02},
	journal = {TensorFlow},
}

@misc{noauthor_onnx_nodate,
	title = {{ONNX} {\textbar} {Home}},
	url = {https://onnx.ai/},
	urldate = {2021-10-02},
}

@article{luo_comparison_2020,
	title = {Comparison and {Benchmarking} of {AI} {Models} and {Frameworks} on {Mobile} {Devices}},
	url = {http://arxiv.org/abs/2005.05085},
	abstract = {Due to increasing amounts of data and compute resources, deep learning achieves many successes in various domains. The application of deep learning on the mobile and embedded devices is taken more and more attentions, benchmarking and ranking the AI abilities of mobile and embedded devices becomes an urgent problem to be solved. Considering the model diversity and framework diversity, we propose a benchmark suite, AIoTBench, which focuses on the evaluation of the inference abilities of mobile and embedded devices. AIoTBench covers three typical heavy-weight networks: ResNet50, InceptionV3, DenseNet121, as well as three light-weight networks: SqueezeNet, MobileNetV2, MnasNet. Each network is implemented by three frameworks which are designed for mobile and embedded devices: Tensorflow Lite, Caffe2, PyTorch Mobile. To compare and rank the AI capabilities of the devices, we propose two unified metrics as the AI scores: Valid Images Per Second (VIPS) and Valid FLOPs Per Second (VOPS). Currently, we have compared and ranked 5 mobile devices using our benchmark. This list will be extended and updated soon after.},
	urldate = {2021-10-02},
	author = {Luo, Chunjie and He, Xiwen and Zhan, Jianfeng and Wang, Lei and Gao, Wanling and Dai, Jiahui},
	month = may,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Performance, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{febvay_low-level_2020,
	address = {Seattle WA USA},
	title = {Low-level {Optimizations} for {Faster} {Mobile} {Deep} {Learning} {Inference} {Frameworks}},
	isbn = {978-1-4503-7988-5},
	url = {https://dl.acm.org/doi/10.1145/3394171.3416516},
	doi = {10.1145/3394171.3416516},
	abstract = {Over the last ten years, we have seen a strong progression of technology around smartphones. Each new generation acquires capabilities that significantly increase performance. On the other hand, several deep learning tools are offered today by the giants of the net for mobile, embedded devices and IoT. The proposed libraries allow a machine learning inference on the device with low latency. They provide pre-trained models, but one can also use one’s own models and run them on mobile, embedded or microcontroller devices. Lack of privacy, poor Internet connectivity and high cost of cloud platform let on-device inference became popular through app developers but there are more significant challenges especially for real-time tasks like augmented reality or autonomous driving. This PhD research aims at providing a path for developers to help them choose the best methods and tools to do real-time inference on mobile devices. In this paper, we present the performance benchmark of four popular open-source deep learning inference frameworks used on mobile devices on three different convolutional neural network models. We focus our work on image classification process and particularly on validation image bank of ImageNet 2012 dataset. We try to answer three questions : How does a framework influence model prediction and latency ? Why some frameworks are better in terms of latency/accuracy than others with the same model ? And what are the difficulties to implement these frameworks inside a mobile application ? Our first findings demonstrate that low-level software implementations chosen in frameworks, model conversion steps and parameters set in the framework have a big impact on performance and accuracy.},
	language = {en},
	urldate = {2021-10-02},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Febvay, Mathieu},
	month = oct,
	year = {2020},
	pages = {4738--4742},
}

@misc{onnx_onnx_2017,
	title = {{ONNX} {Tutorials}},
	copyright = {Apache-2.0},
	author = {ONNX},
	year         = {2017},
	note = {\url{https://github.com/onnx/tutorials}},
	abstract = {Tutorials for creating and using ONNX models},
	urldate = {2021-10-02},
	publisher = {Open Neural Network Exchange},
}

@misc{noauthor_api_nodate,
	title = {{API} {Documentation} {\textbar} {TensorFlow} {Core} v2.6.0},
	url = {https://www.tensorflow.org/api_docs?hl=nl},
	abstract = {An open source machine learning library for research and production.},
	language = {en},
	urldate = {2021-10-02},
}

@article{abadi_tensorflow_2016,
	title = {{TensorFlow}: {A} system for large-scale machine learning},
	shorttitle = {{TensorFlow}},
	url = {http://arxiv.org/abs/1605.08695},
	abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
	urldate = {2021-10-02},
	author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	month = may,
	year = {2016},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing},
	annote = {Comment: 18 pages, 9 figures; v2 has a spelling correction in the metadata},
}

@article{li_PyTorch_2020,
	title = {{PyTorch} distributed: experiences on accelerating data parallel training},
	volume = {13},
	issn = {2150-8097},
	shorttitle = {{PyTorch} distributed},
	url = {https://dl.acm.org/doi/10.14778/3415478.3415530},
	doi = {10.14778/3415478.3415530},
	abstract = {This paper presents the design, implementation, and evaluation of the PyTorch distributed data parallel module. PyTorch is a widely-adopted scientiﬁc computing package used in deep learning research and applications. Recent advances in deep learning argue for the value of large datasets and large models, which necessitates the ability to scale out model training to more computational resources. Data parallelism has emerged as a popular solution for distributed training thanks to its straightforward principle and broad applicability. In general, the technique of distributed data parallelism replicates the model on every computational resource to generate gradients independently and then communicates those gradients at each iteration to keep model replicas consistent. Despite the conceptual simplicity of the technique, the subtle dependencies between computation and communication make it non-trivial to optimize the distributed training eﬃciency. As of v1.5, PyTorch natively provides several techniques to accelerate distributed data parallel, including bucketing gradients, overlapping computation with communication, and skipping gradient synchronization. Evaluations show that, when conﬁgured appropriately, the PyTorch distributed data parallel module attains near-linear scalability using 256 GPUs.},
	language = {en},
	number = {12},
	urldate = {2021-10-02},
	journal = {Proceedings of the VLDB Endowment},
	author = {Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and Chintala, Soumith},
	month = aug,
	year = {2020},
	pages = {3005--3018},
}

@article{lavin_fast_2015,
	title = {Fast {Algorithms} for {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1509.09308},
	abstract = {Deep convolutional neural networks take GPU days of compute time to train on large data sets. Pedestrian detection for self driving cars requires very low latency. Image recognition for mobile phones is constrained by limited processing resources. The success of convolutional neural networks in these situations is limited by how fast we can compute them. Conventional FFT based convolution is fast for large filters, but state of the art convolutional neural networks use small, 3x3 filters. We introduce a new class of fast algorithms for convolutional neural networks using Winograd's minimal filtering algorithms. The algorithms compute minimal complexity convolution over small tiles, which makes them fast with small filters and small batch sizes. We benchmark a GPU implementation of our algorithm with the VGG network and show state of the art throughput at batch sizes from 1 to 64.},
	urldate = {2021-10-04},
	author = {Lavin, Andrew and Gray, Scott},
	month = nov,
	year = {2015},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, F.2.1, I.2.6},
}

@misc{noauthor_object_nodate,
	title = {Object {Detection} with {TensorFlow} {Lite} {Model} {Maker}},
	url = {https://www.tensorflow.org/lite/tutorials/model_maker_object_detection?hl=nl},
	language = {en},
	urldate = {2021-10-19},
}

@misc{Android_NNAPI_2021,
	title = {Neural {Networks} {API} {\textbar} {Android} {NDK}},
	note = {\url{https://developer.android.com/ndk/guides/neuralnetworks?hl=nl}},
	language = {en},
	urldate = {2021-10-19},
	journal = {Android Developers},
	year = {2021},
	author = {Android},
}

@misc{noauthor_dropout_nodate,
	title = {Dropout — {PyTorch} 1.9.1 documentation},
	url = {https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html},
	urldate = {2021-10-20},
}

@misc{onnx_tf2onnx_2021,
	title = {tf2onnx - {Convert} {TensorFlow}, {Keras}, {Tensorflow}.js and {Tflite} models to {ONNX}.},
	copyright = {Apache-2.0},
	note = {\url{https://github.com/onnx/tensorflow-onnx/blob/42e800dc2945e5cadb9df4f09670f2e20eb6d222/support_status.md}},
	abstract = {Convert TensorFlow, Keras, Tensorflow.js and Tflite models to ONNX},
	urldate = {2021-10-21},
	publisher = {Open Neural Network Exchange},
	month = oct,
	year = {2021},
	author = {ONNX},
}

@misc{noauthor_torchonnx_nodate,
	title = {torch.onnx — {PyTorch} 1.9.1 documentation},
	url = {https://pytorch.org/docs/stable/onnx.html#functions},
	urldate = {2021-10-21},
}

@misc{noauthor_torchscript_nodate,
	title = {{TorchScript} — {PyTorch} 1.9.1 documentation},
	url = {https://pytorch.org/docs/stable/jit.html},
	urldate = {2021-10-21},
}

@misc{Google_flatbuffers_2014,
	title = {{FlatBuffers}: Memory Efficient {Serialization} {Library}},
	note = {\url{https://google.github.io/flatbuffers/}},
	urldate = {2021-10-21},
	author = {Google},
	year = {2014},
	publisher = {Github},
}

@misc{Google_protocol_2014,
	title = {Protocol {Buffers} - {Google}'s data interchange format},
	note = {\url{https://github.com/protocolbuffers/protobuf}},
	abstract = {Protocol Buffers - Google's data interchange format},
	urldate = {2021-10-21},
	publisher = {Protocol Buffers},
	month = oct,
	year = {2021},
	keywords = {marshalling, protobuf, protobuf-runtime, protoc, protocol-buffers, protocol-compiler, rpc, serialization},
	author = {Google},
}

@misc{noauthor_prototype_nodate,
	title = {({Prototype}) {Convert} {MobileNetV2} to {NNAPI} — {PyTorch} {Tutorials} 1.9.1+cu102 documentation},
	url = {https://pytorch.org/tutorials/prototype/nnapi_mobilenetv2.html},
	urldate = {2021-10-21},
}

@online{chollet2015keras,
  title={Keras},
  author={Chollet, Francois and others},
  year={2015},
  publisher={GitHub},
  note = {\url{https://github.com/fchollet/keras}},
}

@misc{noauthor_modelsresearchobject_detection_nodate,
	title = {TensorFlow Object Detection API},
	url = {https://github.com/tensorflow/models},
	abstract = {Models and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub.},
	language = {en},
	urldate = {2021-10-22},
	journal = {GitHub},
}

@misc{noauthor_welcome_2021,
	title = {Welcome to the {Model} {Garden} for {TensorFlow}},
	copyright = {Apache-2.0},
	url = {https://github.com/tensorflow/models/blob/5222293580318ec916126b739f5d316072cf1bdf/research/object_detection/g3doc/running_on_mobile_tf2.md},
	abstract = {Models and examples built with TensorFlow},
	urldate = {2021-10-22},
	publisher = {tensorflow},
	month = oct,
	year = {2021},
	note = {original-date: 2016-02-05T01:15:20Z},
}

@misc{noauthor_tensorflow_nodate-1,
	title = {{TensorFlow} graph optimization with {Grappler} {\textbar} {TensorFlow} {Core}},
	url = {https://www.tensorflow.org/guide/graph_optimization?hl=nl},
	language = {en},
	urldate = {2021-10-22},
	journal = {TensorFlow},
}

@misc{noauthor_tensorflow_nodate-2,
	title = {{TensorFlow} {Lite} {Model} {Maker}},
	url = {https://www.tensorflow.org/lite/guide/model_maker},
	urldate = {2021-10-22},
}

@misc{noauthor_nnapi_nodate,
	title = {{NNAPI}},
	url = {https://onnxruntime.ai/docs/execution-providers/NNAPI-ExecutionProvider.html},
	abstract = {ONNX Runtime: cross-platform, high performance ML inferencing and training accelerator},
	language = {en-US},
	urldate = {2021-10-23},
	journal = {onnxruntime},
}

@article{noble_what_2006,
	title = {What is a support vector machine?},
	volume = {24},
	issn = {1546-1696},
	url = {http://www.nature.com/articles/nbt1206-1565},
	doi = {10.1038/nbt1206-1565},
	abstract = {Support vector machines (SVMs) are becoming popular in a wide variety of biological applications. But, what exactly are SVMs and how do they work? And what are their most promising applications in the life sciences?},
	language = {en},
	number = {12},
	urldate = {2021-10-23},
	journal = {Nat Biotechnol},
	author = {Noble, William S.},
	month = dec,
	year = {2006},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 12
Primary\_atype: Reviews
Publisher: Nature Publishing Group},
	keywords = {Agriculture, Bioinformatics, Biomedical Engineering/Biotechnology, Biomedicine, Biotechnology, general, Life Sciences},
	pages = {1565--1567},
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{duan_centernet_2019,
	title = {{CenterNet}: {Keypoint} {Triplets} for {Object} {Detection}},
	shorttitle = {{CenterNet}},
	abstract = {In object detection, keypoint-based approaches often suffer a large number of incorrect object bounding boxes, arguably due to the lack of an additional look into the cropped regions. This paper presents an efficient solution which explores the visual patterns within each cropped region with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules named cascade corner pooling and center pooling, which play the roles of enriching information collected by both top-left and bottom-right corners and providing more recognizable information at the central regions, respectively. On the MS-COCO dataset, CenterNet achieves an AP of 47.0\%, which outperforms all existing one-stage detectors by at least 4.9\%. Meanwhile, with a faster inference speed, CenterNet demonstrates quite comparable performance to the top-ranked two-stage detectors. Code is available at https://github.com/Duankaiwen/CenterNet.},
	urldate = {2021-10-24},
	author = {Duan, Kaiwen and Bai, Song and Xie, Lingxi and Qi, Honggang and Huang, Qingming and Tian, Qi},
	month = apr,
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 10 pages (including 2 pages of References), 7 figures, 5 tables},
}

@article{lin_focal_2018,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://arxiv.org/abs/1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	urldate = {2021-10-24},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = feb,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{tan_efficientdet_2020,
	title = {{EfficientDet}: {Scalable} and {Efficient} {Object} {Detection}},
	shorttitle = {{EfficientDet}},
	url = {http://arxiv.org/abs/1911.09070},
	abstract = {Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single model and single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs, being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detectors. Code is available at https://github.com/google/automl/tree/master/efficientdet.},
	urldate = {2021-10-24},
	author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
	month = jul,
	year = {2020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: CVPR 2020},
}

@misc{noauthor_tensorflow_nodate-3,
	title = {{TensorFlow} {Lite}},
	url = {https://www.tensorflow.org/lite/},
	language = {en},
	urldate = {2021-10-24},
}

@misc{noauthor_model_nodate,
	title = {Model optimization {\textbar} {TensorFlow} {Lite}},
	url = {https://www.tensorflow.org/lite/performance/model_optimization?hl=nl},
	language = {en},
	urldate = {2021-10-24},
	journal = {TensorFlow},
}

@misc{noauthor_tensorflow_nodate-4,
	title = {{TensorFlow} {Lite} {Delegates}},
	url = {https://www.tensorflow.org/lite/performance/delegates?hl=nl},
	language = {en},
	urldate = {2021-10-24},
	journal = {TensorFlow},
}

@misc{PyTorch_torchvision_2017,
	title = {torchvision — {Torchvision} documentation},
	url = {https://pytorch.org/vision/stable/index.html},
	urldate = {2021-10-24},
	author = {PyTorch},
	year = {2017},
}

@article{sandler_mobilenetv2_2019,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	shorttitle = {{MobileNetV2}},
	url = {http://arxiv.org/abs/1801.04381},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
	urldate = {2021-10-24},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	month = mar,
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{noauthor_torchscript_nodate-1,
	title = {{TorchScript} {Builtins} — {PyTorch} 1.10.0 documentation},
	url = {https://pytorch.org/docs/stable/jit_builtin_functions.html#builtin-functions},
	urldate = {2021-10-24},
}

@article{chen_mmdetection_2019,
	title = {{MMDetection}: {Open} {MMLab} {Detection} {Toolbox} and {Benchmark}},
	shorttitle = {{MMDetection}},
	url = {http://arxiv.org/abs/1906.07155},
	abstract = {We present MMDetection, an object detection toolbox that contains a rich set of object detection and instance segmentation methods as well as related components and modules. The toolbox started from a codebase of MMDet team who won the detection track of COCO Challenge 2018. It gradually evolves into a unified platform that covers many popular detection methods and contemporary modules. It not only includes training and inference codes, but also provides weights for more than 200 network models. We believe this toolbox is by far the most complete detection toolbox. In this paper, we introduce the various features of this toolbox. In addition, we also conduct a benchmarking study on different methods, components, and their hyper-parameters. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new detectors. Code and models are available at https://github.com/open-mmlab/mmdetection. The project is under active development and we will keep this document updated.},
	urldate = {2021-10-24},
	author = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and Liu, Ziwei and Xu, Jiarui and Zhang, Zheng and Cheng, Dazhi and Zhu, Chenchen and Cheng, Tianheng and Zhao, Qijie and Li, Buyu and Lu, Xin and Zhu, Rui and Wu, Yue and Dai, Jifeng and Wang, Jingdong and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},
	month = jun,
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: Technical report of MMDetection. 11 pages},
}

@misc{Facebook_detectron2_2021,
	title = {facebookresearch/detectron2},
	copyright = {Apache-2.0},
	note = {\url{https://github.com/facebookresearch/detectron2}},
	abstract = {Detectron2 is FAIR's next-generation platform for object detection, segmentation and other visual recognition tasks.},
	urldate = {2021-10-24},
	publisher = {Facebook Research},
	month = oct,
	year = {2021},
	note = {original-date: 2019-09-05T21:30:20Z},
}

@misc{wu2019detectron2,
  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
                  Wan-Yen Lo and Ross Girshick},
  title =        {Detectron2},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  year =         {2019}
}

@misc{noauthor_d2go_nodate,
	title = {{D2Go} brings {Detectron2} to mobile},
	url = {https://ai.facebook.com/blog/d2go-brings-detectron2-to-mobile/},
	abstract = {Detectron2Go (D2Go) is a new, state-of-the-art extension for Detectron2 that gives developers an end-to-end pipeline for training and deploying object detection models on mobile devices and hardware.},
	language = {nl},
	urldate = {2021-10-24},
}

@misc{noauthor_gluoncv_nodate,
	title = {{GluonCV}: a {Deep} {Learning} {Toolkit} for {Computer} {Vision} — gluoncv 0.11.0 documentation},
	url = {https://cv.gluon.ai/contents.html},
	urldate = {2021-10-24},
}

@article{guo_gluoncv_2020,
	title = {{GluonCV} and {GluonNLP}: {Deep} {Learning} in {Computer} {Vision} and {Natural} {Language} {Processing}},
	shorttitle = {{GluonCV} and {GluonNLP}},
	url = {http://arxiv.org/abs/1907.04433},
	abstract = {We present GluonCV and GluonNLP, the deep learning toolkits for computer vision and natural language processing based on Apache MXNet (incubating). These toolkits provide state-of-the-art pre-trained models, training scripts, and training logs, to facilitate rapid prototyping and promote reproducible research. We also provide modular APIs with flexible building blocks to enable efficient customization. Leveraging the MXNet ecosystem, the deep learning models in GluonCV and GluonNLP can be deployed onto a variety of platforms with different programming languages. The Apache 2.0 license has been adopted by GluonCV and GluonNLP to allow for software distribution, modification, and usage.},
	urldate = {2021-10-24},
	author = {Guo, Jian and He, He and He, Tong and Lausen, Leonard and Li, Mu and Lin, Haibin and Shi, Xingjian and Wang, Chenguang and Xie, Junyuan and Zha, Sheng and Zhang, Aston and Zhang, Hang and Zhang, Zhi and Zhang, Zhongyue and Zheng, Shuai and Zhu, Yi},
	month = feb,
	year = {2020},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{martins_imageai_2021,
	address = {Cham},
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {{ImageAI}: {Comparison} {Study} on {Different} {Custom} {Image} {Recognition} {Algorithms}},
	isbn = {978-3-030-72651-5},
	shorttitle = {{ImageAI}},
	doi = {10.1007/978-3-030-72651-5_57},
	abstract = {Today there are a considerable amount of algorithms that are used by scientists and developers to detect patterns in pictures. Due to the complexity of such analysis, with this paper, we want to understand and share the results of doing a model training based of a pool of pre-selected algorithms recurring to an accessible python library named ImageAI and the public cloud to perform such training. We used the Google platform Colaboraty to execute our tests.},
	language = {en},
	booktitle = {Trends and {Applications} in {Information} {Systems} and {Technologies}},
	publisher = {Springer International Publishing},
	author = {Martins, Manuel and Mota, David and Morgado, Francisco and Wanzeller, Cristina and Martins, Pedro and Abbasi, Maryam},
	editor = {Rocha, Álvaro and Adeli, Hojjat and Dzemyda, Gintautas and Moreira, Fernando and Ramalho Correia, Ana Maria},
	year = {2021},
	keywords = {Custom image recognition, DenseNet, Google colab, Image AI, InceptionV3, Machine learning, Model training, ResNet, SqueezeNet},
	pages = {602--610},
}

@misc{noauthor_official_nodate,
	title = {Official {English} {Documentation} for {ImageAI}! — {ImageAI} 2.1.6 documentation},
	url = {https://imageai.readthedocs.io/en/latest/},
	urldate = {2021-10-24},
}

@misc{onnx_onnxruntime_2019,
	title = {{ONNX} {Runtime} ({ORT})},
	note = {\url{https://onnxruntime.ai/docs/}},
	abstract = {ONNX Runtime: cross-platform, high performance ML inferencing and training accelerator},
	language = {en-US},
	urldate = {2021-10-24},
	journal = {onnxruntime},
	author = {ONNX},
	year   = {2019},
}

@misc{noauthor_tf2onnx_2021-1,
	title = {tf2onnx - {Convert} {TensorFlow}, {Keras}, {Tensorflow}.js and {Tflite} models to {ONNX}.},
	copyright = {Apache-2.0},
	url = {https://github.com/onnx/tensorflow-onnx/blob/42e800dc2945e5cadb9df4f09670f2e20eb6d222/support_status.md},
	abstract = {Convert TensorFlow, Keras, Tensorflow.js and Tflite models to ONNX},
	urldate = {2021-10-24},
	publisher = {Open Neural Network Exchange},
	month = oct,
	year = {2021},
}

@misc{Apple_CoreML_2018,
	title = {Core {ML} {\textbar} {Apple} {Developer} {Documentation}},
	note = {\url{https://developer.apple.com/documentation/coreml}},
	urldate = {2021-10-24},
	author = {Apple},
	year = {2018},
}

@misc{noauthor_introduction_2018,
	title = {Introduction — {MACE} documentation},
	url = {https://mace.readthedocs.io/en/latest/introduction.html},
	urldate = {2021-10-24},
	year = {2018},
}

@misc{khan_mace_2020,
	title = {{MACE}: {Deep} learning optimized for mobile and edge devices},
	shorttitle = {{MACE}},
	note = {\url{https://heartbeat.comet.ml/mace-deep-learning-optimized-for-mobile-and-edge-devices-5e6941cc0533}},
	abstract = {A breakdown of Xiaomi’s take on a deep learning inference engine, and a short guide on how you can use it to build your very own edge-ready},
	language = {en},
	journal = {Medium},
	author = {Khan, Jamshed},
	year = {2020},
}

@misc{goel_survey_2020,
      title={A Survey of Methods for Low-Power Deep Learning and Computer Vision}, 
      author={Abhinav Goel and Caleb Tung and Yung-Hsiang Lu and George K. Thiruvathukal},
      year={2020},
      archivePrefix={arXiv},
}

@misc{noauthor_onnx_nodate-2,
	title = {{ONNX} {\textbar} {Supported} {Tools}},
	url = {https://onnx.ai/supported-tools.html},
	urldate = {2021-10-24},
}

@misc{redmon_yolov3_2018,
      title={YOLOv3: An Incremental Improvement}, 
      author={Joseph Redmon and Ali Farhadi},
	  note = {\url{https://arxiv.org/abs/1804.02767}},
      year={2018},
      eprint={1804.02767},
      archivePrefix={arXiv},
}

@article{Gai_TinyV3_2021,
author = {Wendong Gai and Yakun Liu and Jing Zhang and Gang Jing},
title = {An improved Tiny YOLOv3 for real-time object detection},
journal = {Systems Science \& Control Engineering},
volume = {9},
number = {1},
pages = {314-321},
year  = {2021},
publisher = {Taylor & Francis},
}

@misc{lavin_fast_2015,
      title={Fast Algorithms for Convolutional Neural Networks}, 
      author={Andrew Lavin and Scott Gray},
      year={2015},
      archivePrefix={arXiv},
}

@ARTICLE{Geiger_IJRR_2013,
  author = {Andreas Geiger and Philip Lenz and Christoph Stiller and Raquel Urtasun},
  title = {Vision meets Robotics: The KITTI Dataset},
  journal = {International Journal of Robotics Research (IJRR)},
  year = {2013}
}

@misc{chen_mxnet_2015,
      title={MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems}, 
      author={Tianqi Chen and Mu Li and Yutian Li and Min Lin and Naiyan Wang and Minjie Wang and Tianjun Xiao and Bing Xu and Chiyuan Zhang and Zheng Zhang},
      year={2015},
      eprint={1512.01274},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@misc{nwankpa_activation_2018,
      title={Activation Functions: Comparison of trends in Practice and Research for Deep Learning}, 
      author={Chigozie Nwankpa and Winifred Ijomah and Anthony Gachagan and Stephen Marshall},
      year={2018},
      archivePrefix={arXiv},
}

@ARTICLE{8010421,
  author={Ma, Hongxing and Gou, Jianping and Wang, Xili and Ke, Jia and Zeng, Shaoning},
  journal={IEEE Access}, 
  title={Sparse Coefficient-Based ${k}$ -Nearest Neighbor Classification}, 
  year={2017},
  volume={5},
  number={},
}

@article{Krizhevsky_act_2017,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	year = {2017},
	issue_date = {June 2017},
	publisher = {Association for Computing Machinery},
	volume = {60},
	url = {https://doi-org.kuleuven.e-bronnen.be/10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million
	high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different
	classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%,
	respectively, which is considerably better than the previous state-of-the-art. The
	neural network, which has 60 million parameters and 650,000 neurons, consists of five
	convolutional layers, some of which are followed by max-pooling layers, and three
	fully connected layers with a final 1000-way softmax. To make training faster, we
	used non-saturating neurons and a very efficient GPU implementation of the convolution
	operation. To reduce overfitting in the fully connected layers we employed a recently
	developed regularization method called "dropout" that proved to be very effective.
	We also entered a variant of this model in the ILSVRC-2012 competition and achieved
	a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best	
	entry.},
	journal = {Commun. ACM},
	month = may,
}

@misc{lin2015microsoft,
      title={Microsoft COCO: Common Objects in Context}, 
      author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},
      year={2015},
      eprint={1405.0312},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}


@misc{pytorch_atensrcatennative_2021,
	title = {pytorch/aten/src/{ATen}/native at master · pytorch/pytorch},
	note = {\url{https://github.com/pytorch/pytorch}},
	abstract = {Tensors and Dynamic neural networks in Python with strong GPU acceleration - pytorch/aten/src/ATen/native at master · pytorch/pytorch},
	language = {en},
	urldate = {2021-10-29},
	year = {2021},
	journal = {GitHub},
	author = {PyTorch},
}

@misc{simonyan2015deep,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      note = {\url{https://arxiv.org/abs/1409.1556}},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{he2015deep,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
